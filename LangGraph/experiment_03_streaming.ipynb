{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b3590a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c89364cb",
   "metadata": {},
   "source": [
    "> https://docs.langchain.com/oss/python/langgraph/streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f88daf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "135b2724",
   "metadata": {},
   "source": [
    "# Basic usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a87f72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'refine_topic': {'topic': 'ice cream and cats'}}\n",
      "{'generate_joke': {'joke': 'This is a joke about ice cream and cats'}}\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "\n",
    "def refine_topic(state: State):\n",
    "    return {\"topic\": state[\"topic\"] + \" and cats\"}\n",
    "\n",
    "def generate_joke(state: State):\n",
    "    return {\"joke\": f\"This is a joke about {state['topic']}\"}\n",
    "\n",
    "graph = (\n",
    "    StateGraph(State)\n",
    "    .add_node(refine_topic)\n",
    "    .add_node(generate_joke)\n",
    "    .add_edge(START, \"refine_topic\")\n",
    "    .add_edge(\"refine_topic\", \"generate_joke\")\n",
    "    .add_edge(\"generate_joke\", END)\n",
    "    .compile()\n",
    ")\n",
    "\n",
    "# The stream() method returns an iterator that yields streamed outputs\n",
    "for chunk in graph.stream(  \n",
    "    {\"topic\": \"ice cream\"},\n",
    "    # Set stream_mode=\"updates\" to stream only the updates to the graph state after each node\n",
    "    # Other stream modes are also available. See supported stream modes for details\n",
    "    stream_mode=\"updates\",  \n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448b0106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94a709eb",
   "metadata": {},
   "source": [
    "# Stream multiple modes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24da35b",
   "metadata": {},
   "source": [
    "You can pass a list as the `stream_mode` parameter to stream multiple modes at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dadb8a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values {'topic': 'ice cream'}\n",
      "updates {'refine_topic': {'topic': 'ice cream and cats'}}\n",
      "values {'topic': 'ice cream and cats'}\n",
      "updates {'generate_joke': {'joke': 'This is a joke about ice cream and cats'}}\n",
      "values {'topic': 'ice cream and cats', 'joke': 'This is a joke about ice cream and cats'}\n"
     ]
    }
   ],
   "source": [
    "for mode, chunk in graph.stream({\"topic\": \"ice cream\"}, stream_mode=[\"updates\", \"values\"]):\n",
    "    print(mode, chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69a3394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3dcdd78",
   "metadata": {},
   "source": [
    "# Stream subgraph outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4edb4a7",
   "metadata": {},
   "source": [
    "To include outputs from subgraphs in the streamed outputs, you can set `subgraphs=True` in the `.stream()` method of the parent graph. This will stream outputs from both the parent graph and any subgraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2616b487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((), {'node_1': {'foo': 'hi! foo'}})\n",
      "(('node_2:509ef785-d223-e58f-b95a-fba73454082e',), {'subgraph_node_1': {'bar': 'bar'}})\n",
      "(('node_2:509ef785-d223-e58f-b95a-fba73454082e',), {'subgraph_node_2': {'foo': 'hi! foobar'}})\n",
      "((), {'node_2': {'foo': 'hi! foobar'}})\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing import TypedDict\n",
    "\n",
    "# Define subgraph\n",
    "class SubgraphState(TypedDict):\n",
    "    foo: str  # note that this key is shared with the parent graph state\n",
    "    bar: str\n",
    "\n",
    "def subgraph_node_1(state: SubgraphState):\n",
    "    return {\"bar\": \"bar\"}\n",
    "\n",
    "def subgraph_node_2(state: SubgraphState):\n",
    "    return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n",
    "\n",
    "subgraph_builder = StateGraph(SubgraphState)\n",
    "subgraph_builder.add_node(subgraph_node_1)\n",
    "subgraph_builder.add_node(subgraph_node_2)\n",
    "subgraph_builder.add_edge(START, \"subgraph_node_1\")\n",
    "subgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\n",
    "subgraph = subgraph_builder.compile()\n",
    "\n",
    "# Define parent graph\n",
    "class ParentState(TypedDict):\n",
    "    foo: str\n",
    "\n",
    "def node_1(state: ParentState):\n",
    "    return {\"foo\": \"hi! \" + state[\"foo\"]}\n",
    "\n",
    "builder = StateGraph(ParentState)\n",
    "builder.add_node(\"node_1\", node_1)\n",
    "builder.add_node(\"node_2\", subgraph)\n",
    "builder.add_edge(START, \"node_1\")\n",
    "builder.add_edge(\"node_1\", \"node_2\")\n",
    "graph = builder.compile()\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    {\"foo\": \"foo\"},\n",
    "    stream_mode=\"updates\",\n",
    "    # Set subgraphs=True to stream outputs from subgraphs\n",
    "    subgraphs=True,  \n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137a9e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "329302c3",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ba0808",
   "metadata": {},
   "source": [
    "Use the `debug` streaming mode to stream as much information as possible throughout the execution of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "baf091e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((), {'step': 1, 'timestamp': '2025-11-04T09:47:57.661934+00:00', 'type': 'task', 'payload': {'id': 'bf0f9ffc-f2ad-6b85-55fd-f7bcf6490c4e', 'name': 'node_1', 'input': {'foo': 'foo'}, 'triggers': ('branch:to:node_1',)}})\n",
      "((), {'step': 1, 'timestamp': '2025-11-04T09:47:57.663390+00:00', 'type': 'task_result', 'payload': {'id': 'bf0f9ffc-f2ad-6b85-55fd-f7bcf6490c4e', 'name': 'node_1', 'error': None, 'result': {'foo': 'hi! foo'}, 'interrupts': []}})\n",
      "((), {'step': 2, 'timestamp': '2025-11-04T09:47:57.663629+00:00', 'type': 'task', 'payload': {'id': 'e0780287-fa09-2fe2-0ad3-efeb6f1a80e6', 'name': 'node_2', 'input': {'foo': 'hi! foo'}, 'triggers': ('branch:to:node_2',)}})\n",
      "(('node_2:e0780287-fa09-2fe2-0ad3-efeb6f1a80e6',), {'step': 1, 'timestamp': '2025-11-04T09:47:57.665100+00:00', 'type': 'task', 'payload': {'id': '668f58ba-9245-2c0c-cd13-48b9a0ee89dc', 'name': 'subgraph_node_1', 'input': {'foo': 'hi! foo'}, 'triggers': ('branch:to:subgraph_node_1',)}})\n",
      "(('node_2:e0780287-fa09-2fe2-0ad3-efeb6f1a80e6',), {'step': 1, 'timestamp': '2025-11-04T09:47:57.666169+00:00', 'type': 'task_result', 'payload': {'id': '668f58ba-9245-2c0c-cd13-48b9a0ee89dc', 'name': 'subgraph_node_1', 'error': None, 'result': {'bar': 'bar'}, 'interrupts': []}})\n",
      "(('node_2:e0780287-fa09-2fe2-0ad3-efeb6f1a80e6',), {'step': 2, 'timestamp': '2025-11-04T09:47:57.666296+00:00', 'type': 'task', 'payload': {'id': '194500dd-89c7-15d0-e187-c7b44a5858e4', 'name': 'subgraph_node_2', 'input': {'foo': 'hi! foo', 'bar': 'bar'}, 'triggers': ('branch:to:subgraph_node_2',)}})\n",
      "(('node_2:e0780287-fa09-2fe2-0ad3-efeb6f1a80e6',), {'step': 2, 'timestamp': '2025-11-04T09:47:57.667021+00:00', 'type': 'task_result', 'payload': {'id': '194500dd-89c7-15d0-e187-c7b44a5858e4', 'name': 'subgraph_node_2', 'error': None, 'result': {'foo': 'hi! foobar'}, 'interrupts': []}})\n",
      "((), {'step': 2, 'timestamp': '2025-11-04T09:47:57.667560+00:00', 'type': 'task_result', 'payload': {'id': 'e0780287-fa09-2fe2-0ad3-efeb6f1a80e6', 'name': 'node_2', 'error': None, 'result': {'foo': 'hi! foobar'}, 'interrupts': []}})\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\"foo\": \"foo\"},\n",
    "    stream_mode=\"debug\",\n",
    "    # Set subgraphs=True to stream outputs from subgraphs\n",
    "    subgraphs=True,  \n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d203da79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28565c90",
   "metadata": {},
   "source": [
    "# LLM Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3cf06d",
   "metadata": {},
   "source": [
    "Use the `messages` streaming mode to stream Large Language Model (LLM) outputs token by token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd16d42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why| did| the| ice| cream| truck| break| down|?\n",
      "\n",
      "|Because| it| had| a| rocky| road|!|"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MyState:\n",
    "    topic: str\n",
    "    joke: str = \"\"\n",
    "\n",
    "\n",
    "model = ChatDeepSeek(model=\"deepseek-chat\")\n",
    "\n",
    "def call_model(state: MyState):\n",
    "    \"\"\"Call the LLM to generate a joke about a topic\"\"\"\n",
    "    # Note that message events are emitted even when the LLM is run using .invoke rather than .stream\n",
    "    model_response = model.invoke(  \n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": f\"Generate a joke about {state.topic}\"}\n",
    "        ]\n",
    "    )\n",
    "    return {\"joke\": model_response.content}\n",
    "\n",
    "graph = (\n",
    "    StateGraph(MyState)\n",
    "    .add_node(call_model)\n",
    "    .add_edge(START, \"call_model\")\n",
    "    .compile()\n",
    ")\n",
    "\n",
    "# The \"messages\" stream mode returns an iterator of tuples (message_chunk, metadata)\n",
    "# where message_chunk is the token streamed by the LLM and metadata is a dictionary\n",
    "# with information about the graph node where the LLM was called and other information\n",
    "for message_chunk, metadata in graph.stream(\n",
    "    {\"topic\": \"ice cream\"},\n",
    "    stream_mode=\"messages\",  \n",
    "):\n",
    "    if message_chunk.content:\n",
    "        print(message_chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47261478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeda3a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a510803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c76b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e796aa49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15192f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
