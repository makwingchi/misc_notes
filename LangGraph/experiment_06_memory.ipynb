{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd60903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c513778b",
   "metadata": {},
   "source": [
    "> https://docs.langchain.com/oss/python/langgraph/add-memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddff83c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9475ad5",
   "metadata": {},
   "source": [
    "# Add short-term memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b9067ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='hi! i am Bob', additional_kwargs={}, response_metadata={}, id='21483160-0c57-4d32-abc3-a45eef0ccb69'),\n",
       "  AIMessage(content='Hi Bob! Nice to meet you. ðŸ˜Š  \\nIs there anything I can help you with today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 9, 'total_tokens': 30, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 9}, 'model_provider': 'deepseek', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'bb4b1281-d06e-49ef-919e-77a41d532239', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--aeb74e1a-62ea-4e18-8059-bbc9164b93df-0', usage_metadata={'input_tokens': 9, 'output_tokens': 21, 'total_tokens': 30, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "model = ChatDeepSeek(model=\"deepseek-chat\", temperature=0.6)\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "checkpointer = InMemorySaver()  \n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(call_model)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"1\"\n",
    "    }\n",
    "}\n",
    "\n",
    "graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi! i am Bob\"}]}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a80e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d31e66f",
   "metadata": {},
   "source": [
    "## Use in subgraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a2add0",
   "metadata": {},
   "source": [
    "If your graph contains subgraphs, you only need to provide the checkpointer when compiling the parent graph. LangGraph will automatically propagate the checkpointer to the child subgraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4d6603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph, END\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from typing import TypedDict\n",
    "\n",
    "class State(TypedDict):\n",
    "    foo: str\n",
    "\n",
    "# Subgraph\n",
    "def subgraph_node_1(state: State):\n",
    "    return {\"foo\": state[\"foo\"] + \"bar\"}\n",
    "\n",
    "subgraph_builder = StateGraph(State)\n",
    "subgraph_builder.add_node(subgraph_node_1)\n",
    "subgraph_builder.add_edge(START, \"subgraph_node_1\")\n",
    "subgraph = subgraph_builder.compile()  \n",
    "\n",
    "# Parent graph\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"node_1\", subgraph)  \n",
    "builder.add_edge(START, \"node_1\")\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "graph = builder.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2e9404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98b821be",
   "metadata": {},
   "source": [
    "# Add long-term memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc7b893",
   "metadata": {},
   "source": [
    "Use long-term memory to store user-specific or application-specific data across conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b01a6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.memory import InMemoryStore  \n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "class FooState(TypedDict):\n",
    "    foo: str\n",
    "\n",
    "def foo_node(state: FooState):\n",
    "    return {\"foo\": state[\"foo\"]}\n",
    "\n",
    "builder = StateGraph(FooState)\n",
    "builder.add_node(foo_node)\n",
    "builder.add_edge(START, \"foo_node\")\n",
    "builder.add_edge(\"foo_node\", END)\n",
    "\n",
    "store = InMemoryStore()\n",
    "graph = builder.compile(store=store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea310f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38ec440c",
   "metadata": {},
   "source": [
    "## Use sementic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c99cae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import init_embeddings\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "# Create store with semantic search enabled\n",
    "embeddings = init_embeddings(\"openai:text-embedding-3-small\")\n",
    "store = InMemoryStore(\n",
    "    index={\n",
    "        \"embed\": embeddings,\n",
    "        \"dims\": 1536,\n",
    "    }\n",
    ")\n",
    "\n",
    "store.put((\"user_123\", \"memories\"), \"1\", {\"text\": \"I love pizza\"})\n",
    "store.put((\"user_123\", \"memories\"), \"2\", {\"text\": \"I am a plumber\"})\n",
    "\n",
    "items = store.search(\n",
    "    (\"user_123\", \"memories\"), query=\"I'm hungry\", limit=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a76ab5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a524f13",
   "metadata": {},
   "source": [
    "# Manage short-term memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7978cf",
   "metadata": {},
   "source": [
    "## Trim messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a21fd353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='fd27e5f0-fedf-41f8-a972-f17f7eb67fa9')]\n",
      "[HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='fd27e5f0-fedf-41f8-a972-f17f7eb67fa9'), AIMessage(content=\"Hi Bob! It's nice to meet you. ðŸ˜Š\\n\\nIs there anything I can help you with today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 10, 'total_tokens': 33, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 10}, 'model_provider': 'deepseek', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': '1e972a5e-8ebe-4c38-81de-6fda0570e449', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--ecfbd90f-d96a-4041-8643-a518ad0df9a1-0', usage_metadata={'input_tokens': 10, 'output_tokens': 23, 'total_tokens': 33, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='write a short poem about cats', additional_kwargs={}, response_metadata={}, id='04eab4f4-20fb-454f-9de6-209d36ff30a5')]\n",
      "[HumanMessage(content='now do the same but for dogs', additional_kwargs={}, response_metadata={}, id='3944839f-9433-484a-b73c-7969c22633d1')]\n",
      "[HumanMessage(content=\"what's my name?\", additional_kwargs={}, response_metadata={}, id='9395bd98-f155-4cd8-9995-d92db1f829a2')]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I donâ€™t have access to your name unless you tell me. In this conversation, you havenâ€™t introduced yourself yet. ðŸ˜Š  \n",
      "If youâ€™d like, you can tell me your name, and Iâ€™ll be happy to use it in our conversation!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages.utils import (\n",
    "    trim_messages,  \n",
    "    count_tokens_approximately  \n",
    ")\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langgraph.graph import StateGraph, START, MessagesState\n",
    "\n",
    "model = ChatDeepSeek(model=\"deepseek-chat\", temperature=0.6)\n",
    "summarization_model = model.bind(max_tokens=128)\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    messages = trim_messages(  \n",
    "        state[\"messages\"],\n",
    "        strategy=\"last\",\n",
    "        token_counter=count_tokens_approximately,\n",
    "        max_tokens=128,\n",
    "        start_on=\"human\",\n",
    "        end_on=(\"human\", \"tool\"),\n",
    "    )\n",
    "    print(messages)\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(call_model)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "graph.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
    "graph.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
    "graph.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
    "final_response = graph.invoke({\"messages\": \"what's my name?\"}, config)\n",
    "\n",
    "final_response[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e995cd12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c6a1030",
   "metadata": {},
   "source": [
    "## Delete messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bca5fe6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('human', \"hi! I'm bob\")]\n",
      "[('human', \"hi! I'm bob\"), ('ai', \"Hi Bob! Nice to meet you. ðŸ˜Š\\n\\nIs there anything I can help you with today? Whether it's answering questions, solving a problem, or just having a chat â€” I'm here for it!\")]\n",
      "2\n",
      "[('human', \"hi! I'm bob\"), ('ai', \"Hi Bob! Nice to meet you. ðŸ˜Š\\n\\nIs there anything I can help you with today? Whether it's answering questions, solving a problem, or just having a chat â€” I'm here for it!\"), ('human', \"what's my name?\")]\n",
      "[('human', \"hi! I'm bob\"), ('ai', \"Hi Bob! Nice to meet you. ðŸ˜Š\\n\\nIs there anything I can help you with today? Whether it's answering questions, solving a problem, or just having a chat â€” I'm here for it!\"), ('human', \"what's my name?\"), ('ai', 'You told me your name is **Bob**! ðŸ˜Š  \\nUnless youâ€™d like me to call you something else?')]\n",
      "4\n",
      "[('human', \"what's my name?\"), ('ai', 'You told me your name is **Bob**! ðŸ˜Š  \\nUnless youâ€™d like me to call you something else?')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import RemoveMessage  \n",
    "\n",
    "def delete_messages(state):\n",
    "    messages = state[\"messages\"]\n",
    "    print(len(messages))\n",
    "    if len(messages) > 2:\n",
    "        # remove the earliest two messages\n",
    "        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}  \n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_sequence([call_model, delete_messages])\n",
    "builder.add_edge(START, \"call_model\")\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "app = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "for event in app.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "    config,\n",
    "    stream_mode=\"values\"\n",
    "):\n",
    "    print([(message.type, message.content) for message in event[\"messages\"]])\n",
    "\n",
    "for event in app.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "    config,\n",
    "    stream_mode=\"values\"\n",
    "):\n",
    "    print([(message.type, message.content) for message in event[\"messages\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050b2e05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c9df40c",
   "metadata": {},
   "source": [
    "## Summarize messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb39ea38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "\n",
    "class State(MessagesState):\n",
    "    summary: str\n",
    "\n",
    "def summarize_conversation(state: State):\n",
    "    # First, we get any existing summary\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # Create our summarization prompt\n",
    "    if summary:\n",
    "        # A summary already exists\n",
    "        summary_message = (\n",
    "            f\"This is a summary of the conversation to date: {summary}\\n\\n\"\n",
    "            \"Extend the summary by taking into account the new messages above:\"\n",
    "        )\n",
    "    else:\n",
    "        summary_message = \"Create a summary of the conversation above:\"\n",
    "\n",
    "    # Add prompt to our history\n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "    response = model.invoke(messages)\n",
    "\n",
    "    # Delete all but the 2 most recent messages\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c492dc34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d953ab88",
   "metadata": {},
   "source": [
    "## Manage checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55493330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6506337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa37a680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d9de71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f16a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55075a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e0f644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
